# Batch, Mini-Batch, Stochastic Gradient Descent Implementation from Scratch - Neural Network

Welcome to the Batch, Mini-Batch, and Stochastic Gradient Descent implementation repository for Neural Networks. This repository contains code from scratch for three different optimization methods: Batch Gradient Descent, Mini-Batch Gradient Descent, and Stochastic Gradient Descent.

## Repository Structure

1. **Code**: This folder contains the core implementation of the three gradient descent methods:
   - `batch_gradient_descent.py`: Code for Batch Gradient Descent
   - `mini_batch_gradient_descent.py`: Code for Mini-Batch Gradient Descent
   - `stochastic_gradient_descent.py`: Code for Stochastic Gradient Descent

2. **Diagram**: This folder includes images of the pipeline, providing a visual representation of the workflow and architecture.

3. **Textual-outputs-and-graphs**: This folder contains explanations and textual outputs for all the implemented codes. It may include graphs and charts to illustrate the performance and convergence of each gradient descent method.

## Implementation Details

In this repository, we have developed the algorithms from scratch for Batch, Mini-Batch, and Stochastic Gradient Descent. These algorithms have been trained and tested on the Boston dataset, a well-known dataset in machine learning.

## How to Use

1. Navigate to the `Code` folder.

2. Open the respective Python file based on the gradient descent method you want to explore or use:
   - `batch_gradient_descent.py`
   - `mini_batch_gradient_descent.py`
   - `stochastic_gradient_descent.py`

3. Run the chosen Python file to see the implementation and outputs.

Feel free to explore the code, diagrams, and textual explanations to gain insights into the different gradient descent methods and how they perform on the Boston dataset.

If you have any questions, suggestions, or issues, please feel free to reach out. Happy coding!
